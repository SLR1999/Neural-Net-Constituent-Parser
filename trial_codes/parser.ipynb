{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSM's code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "import trees\n",
    "import evaluate\n",
    "import vocabulary\n",
    "\n",
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\"\n",
    "UNK = \"<UNK>\"\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "def augment(scores, oracle_index):\n",
    "    \"\"\"Adds the hamming loss to the wrong labels\"\"\"\n",
    "    print(\"(Augment function) Size of scores tensor: {}\".format(scores.size()))\n",
    "    shape = list(scores.size())[0]\n",
    "#     increment = torch.ones(shape)\n",
    "    increment = torch.ones(shape,1)\n",
    "    increment[oracle_index] = 0\n",
    "    print(\"(Augment function) scores: {}\".format(scores))\n",
    "    return scores + Variable(increment)\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        dims = [self.input_dim] + self.hidden_dims + [self.output_dim]\n",
    "        for prev_dim, next_dim in zip(dims, dims[1:]):\n",
    "            self.weights.append(nn.Parameter(torch.zeros(prev_dim, next_dim)))\n",
    "            self.biases.append(nn.Parameter(torch.zeros(next_dim,1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(\"Size of x: {}\".format(x.size()))\n",
    "            print(\"Size of weight: {}\".format(weight.size()))\n",
    "            x = torch.matmul(weight.t(),x)\n",
    "            print(x)\n",
    "            x = torch.add(bias,x)\n",
    "            if i < len(self.weights) - 1:\n",
    "                x = self.relu(x)\n",
    "            print(x)\n",
    "        return x\n",
    "\n",
    "class TopDownParser(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag_vocab,\n",
    "            word_vocab,\n",
    "            label_vocab,\n",
    "            tag_embedding_dim,\n",
    "            word_embedding_dim,\n",
    "            lstm_layers,\n",
    "            lstm_dim,\n",
    "            label_hidden_dim,\n",
    "            split_hidden_dim,\n",
    "            dropout,\n",
    "    ):\n",
    "        super(TopDownParser, self).__init__()\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.tag_embedding_dim = tag_embedding_dim\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.label_hidden_dim = label_hidden_dim\n",
    "        self.split_hidden_dim = split_hidden_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.tag_embeddings = nn.Embedding(tag_vocab.size, tag_embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding(word_vocab.size, word_embedding_dim)\n",
    "\n",
    "        print(\"in model:\", self.lstm_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = self.tag_embedding_dim + self.word_embedding_dim,\n",
    "            hidden_size = lstm_dim,\n",
    "            num_layers=self.lstm_layers,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=True\n",
    "            )\n",
    "\n",
    "        self.f_label = Feedforward(2 * lstm_dim, [label_hidden_dim], label_vocab.size)\n",
    "        self.f_split = Feedforward(2 * lstm_dim, [split_hidden_dim], 1)\n",
    "\n",
    "    def forward(self, sentence, gold=None, explore=True):\n",
    "        is_train = gold is not None\n",
    "\n",
    "        if is_train:\n",
    "            #enable dropout in lstm\n",
    "            pass\n",
    "        else:\n",
    "            #disable dropout in lstm\n",
    "            torch.no_grad()\n",
    "\n",
    "        indices = []\n",
    "        for tag, word in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            tag_index = self.tag_vocab.index(tag)\n",
    "            if word not in (START, STOP):\n",
    "                count = self.word_vocab.count(word)\n",
    "                if not count or (is_train and np.random.rand() < 1 / (1 + count)):\n",
    "                    word = UNK\n",
    "            word_index = self.word_vocab.index(word)\n",
    "            indices.append([tag_index, word_index])\n",
    "\n",
    "        indices = torch.LongTensor(indices).t()\n",
    "        print(indices)\n",
    "        embeddings = torch.cat([self.tag_embeddings(indices[0]), \n",
    "                                self.word_embeddings(indices[1])],\n",
    "                                -1)\n",
    "        print(\"Embedding dim: \",embeddings.size())\n",
    "        lstm_outputs, _ = self.lstm(embeddings.unsqueeze(1))\n",
    "        print(\"LSTM output dimension before squeeze {}:\".format(lstm_outputs.size()))\n",
    "\n",
    "        @functools.lru_cache(maxsize=None)\n",
    "        def get_span_encoding(left, right):\n",
    "            forward = (\n",
    "                lstm_outputs[right][0][:self.lstm_dim] -\n",
    "                lstm_outputs[left][0][:self.lstm_dim])\n",
    "            backward = (\n",
    "                lstm_outputs[left + 1][0][self.lstm_dim:] -\n",
    "                lstm_outputs[right + 1][0][self.lstm_dim:])\n",
    "            return torch.tensor(torch.cat((forward,backward),0).tolist())\n",
    "#             return torch.cat([forward, backward])\n",
    "\n",
    "        def helper(left, right):\n",
    "            label_scores = self.f_label(get_span_encoding(left, right))\n",
    "            label_scores.requires_grad_(True)\n",
    "\n",
    "            if is_train:\n",
    "                oracle_label = gold.oracle_label(left, right)\n",
    "                oracle_label_index = self.label_vocab.index(oracle_label)\n",
    "                label_scores = augment(label_scores, oracle_label_index)\n",
    "\n",
    "            label_scores_np = label_scores.data.numpy()\n",
    "            print(\"Label scores: {}\".format(label_scores_np))\n",
    "            argmax_label_index = int(\n",
    "                label_scores_np.argmax() if right - left < len(sentence) else\n",
    "                label_scores_np[1:].argmax() + 1)\n",
    "            print(argmax_label_index)\n",
    "            argmax_label = self.label_vocab.value(argmax_label_index)\n",
    "\n",
    "            if is_train:\n",
    "                label = argmax_label if explore else oracle_label\n",
    "                label_loss = (\n",
    "                    label_scores[argmax_label_index] -\n",
    "                    label_scores[oracle_label_index]\n",
    "                    if argmax_label != oracle_label else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                label = argmax_label\n",
    "                label_loss = label_scores[argmax_label_index]\n",
    "\n",
    "            if right - left == 1:\n",
    "                tag, word = sentence[left]\n",
    "                tree = trees.LeafParseNode(left, tag, word)\n",
    "                if label:\n",
    "                    tree = trees.InternalParseNode(label, [tree])\n",
    "                return [tree], label_loss\n",
    "\n",
    "            left_encodings = []\n",
    "            right_encodings = []\n",
    "            for split in range(left + 1, right):\n",
    "                left_encodings.append(get_span_encoding(left, split).tolist())\n",
    "                right_encodings.append(get_span_encoding(split, right).tolist())\n",
    "                \n",
    "            left_scores = torch.tensor([self.f_split(torch.tensor(encoding)).item() for encoding in left_encodings])\n",
    "            right_scores = torch.tensor([self.f_split(torch.tensor(encoding)).item() for encoding in right_encodings])\n",
    "            split_scores = left_scores + right_scores\n",
    "            split_scores.requires_grad_(True)\n",
    "\n",
    "            #need to check dimensions here\n",
    "#             print(\"(Helper function) Dimension of split encodings left: {}\".format(left_encodings[0].size()))\n",
    "#             left_scores = self.f_split(left_encodings,1)\n",
    "#             right_scores = self.f_split(right_encodings,1)\n",
    "#             split_scores = left_scores + right_scores\n",
    "#             split_scores = split_scores.view(-1, len(left_encodings), 1)\n",
    "            print(\"(Helper function) Dimension of split scores: {}\".format(split_scores.size()))\n",
    "\n",
    "            if is_train:\n",
    "                oracle_splits = gold.oracle_splits(left, right)\n",
    "                oracle_split = min(oracle_splits)\n",
    "                oracle_split_index = oracle_split - (left + 1)\n",
    "                split_scores = augment(split_scores, oracle_split_index)\n",
    "\n",
    "            split_scores_np = split_scores.data.numpy()\n",
    "            argmax_split_index = int(split_scores_np.argmax())\n",
    "            argmax_split = argmax_split_index + (left + 1)\n",
    "\n",
    "            if is_train:\n",
    "                split = argmax_split if explore else oracle_split\n",
    "                split_loss = (\n",
    "                    split_scores[argmax_split_index] -\n",
    "                    split_scores[oracle_split_index]\n",
    "                    if argmax_split != oracle_split else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                split = argmax_split\n",
    "                split_loss = split_scores[argmax_split_index]\n",
    "\n",
    "            left_trees, left_loss = helper(left, split)\n",
    "            right_trees, right_loss = helper(split, right)\n",
    "\n",
    "            children = left_trees + right_trees\n",
    "            if label:\n",
    "                children = [trees.InternalParseNode(label, children)]\n",
    "\n",
    "            return children, label_loss + split_loss + left_loss + right_loss\n",
    "\n",
    "        children, loss = helper(0, len(sentence))\n",
    "        assert len(children) == 1\n",
    "        tree = children[0]\n",
    "        if is_train and not explore:\n",
    "            assert gold.convert().linearize() == tree.convert().linearize()\n",
    "        return tree, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "def format_elapsed(start_time):\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    days, hours = divmod(hours, 24)\n",
    "    elapsed_string = \"{}h{:02}m{:02}s\".format(hours, minutes, seconds)\n",
    "    if days > 0:\n",
    "        elapsed_string = \"{}d{}\".format(days, elapsed_string)\n",
    "    return elapsed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_treebank = trees.load_trees(\"../data/02-21.10way.clean\")\n",
    "#     dev_treebank = trees.load_trees(\"../data/22.auto.clean\")\n",
    "    train_parse = [tree.convert() for tree in train_treebank]\n",
    "    \n",
    "    tag_vocab = vocabulary.Vocabulary()\n",
    "    tag_vocab.index(START)\n",
    "    tag_vocab.index(STOP)\n",
    "\n",
    "    word_vocab = vocabulary.Vocabulary()\n",
    "    word_vocab.index(START)\n",
    "    word_vocab.index(STOP)\n",
    "    word_vocab.index(UNK)\n",
    "\n",
    "    label_vocab = vocabulary.Vocabulary()\n",
    "    label_vocab.index(())\n",
    "\n",
    "    for tree in train_parse:\n",
    "        nodes = [tree]\n",
    "        while nodes:\n",
    "            node = nodes.pop()\n",
    "            if isinstance(node, trees.InternalParseNode):\n",
    "                label_vocab.index(node.label)\n",
    "                nodes.extend(reversed(node.children))\n",
    "            else:\n",
    "                tag_vocab.index(node.tag)\n",
    "                word_vocab.index(node.word)\n",
    "\n",
    "    tag_vocab.freeze()\n",
    "    word_vocab.freeze()\n",
    "    label_vocab.freeze()\n",
    "    \n",
    "    def print_vocabulary(name, vocab):\n",
    "        special = {START, STOP, UNK}\n",
    "        print(\"{} ({:,}): {}\".format(\n",
    "            name, vocab.size,\n",
    "            sorted(value for value in vocab.values if value in special) +\n",
    "            sorted(value for value in vocab.values if value not in special)))\n",
    "\n",
    "    if False:\n",
    "        print_vocabulary(\"Tag\", tag_vocab)\n",
    "        print_vocabulary(\"Word\", word_vocab)\n",
    "        print_vocabulary(\"Label\", label_vocab)\n",
    "        \n",
    "    parser = TopDownParser(\n",
    "        tag_vocab,\n",
    "        word_vocab,\n",
    "        label_vocab,\n",
    "        50, #tag_embedding_dim,\n",
    "        100, #word_embedding_dim,\n",
    "        2, #lstm_layers,\n",
    "        250, #lstm_dim,\n",
    "        250, #label_hidden_dim,\n",
    "        250, #split_hidden_dim,\n",
    "        0.4 #dropout\n",
    "    )\n",
    "#     parser.cuda()\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(parser.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_processed = 0\n",
    "    current_processed = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in itertools.count(start=1):\n",
    "        if 10 is not None and epoch > 10:\n",
    "            break\n",
    "\n",
    "        np.random.shuffle(train_parse)\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for start_index in range(0, len(train_parse), 10):\n",
    "            optimizer.zero_grad()\n",
    "            parser.train()\n",
    "            batch_losses = []\n",
    "            for tree in train_parse[start_index:start_index + 10]:\n",
    "                sentence = [(leaf.tag, leaf.word) for leaf in tree.leaves()]\n",
    "                _, loss = parser.forward(sentence, tree, True)\n",
    "                batch_losses.append(loss)\n",
    "                total_processed += 1\n",
    "                current_processed += 1\n",
    "\n",
    "            batch_loss = torch.stack(batch_losses).mean()\n",
    "            batch_loss_value = batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"epoch {:,} \"\n",
    "                \"batch {:,}/{:,} \"\n",
    "                \"processed {:,} \"\n",
    "                \"batch-loss {:.4f} \"\n",
    "                \"epoch-elapsed {} \"\n",
    "                \"total-elapsed {}\".format(\n",
    "                    epoch,\n",
    "                    start_index // 10 + 1,\n",
    "                    int(np.ceil(len(train_parse) / 10)),\n",
    "                    total_processed,\n",
    "                    batch_loss_value,\n",
    "                    format_elapsed(epoch_start_time),\n",
    "                    format_elapsed(start_time),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in model: 250\n",
      "tensor([[    0,     2,     4,     4,     4,     4,    11,     6,     4,    14,\n",
      "             4,     4,    12,    18,    38,     5,    14,    20,    38,     5,\n",
      "             3,    14,     2,     4,    18,    38,     5,    14,    20,    38,\n",
      "             5,     3,     6,    21,     1],\n",
      "        [    0,     3,   506,   507,  2632,   125,  4295,   327,  3033,    27,\n",
      "          4933,  2067,   180,  9482,   411,  2462,    27,    41,   411, 17235,\n",
      "          1331,    27,   386, 17228,  9715,   411, 17236,    27,    41,   411,\n",
      "         17237,    95,    96,    44,     1]])\n",
      "Embedding dim:  torch.Size([35, 150])\n",
      "LSTM output dimension before squeeze torch.Size([35, 1, 500]):\n",
      "Size of x: torch.Size([500])\n",
      "Size of weight: torch.Size([500, 250])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<MvBackward>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<ReluBackward0>)\n",
      "Size of x: torch.Size([250, 250])\n",
      "Size of weight: torch.Size([250, 113])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<MmBackward>)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>)\n",
      "(Augment function) Size of scores tensor: torch.Size([113, 250])\n",
      "(Augment function) scores: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], grad_fn=<AddBackward0>)\n",
      "Label scores: [[1. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "251\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-68274d7e071c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-c9d24e1c995f>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_parse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mtotal_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-3fd989b97691>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, gold, explore)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-3fd989b97691>\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 label_scores_np[1:].argmax() + 1)\n\u001b[1;32m    153\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margmax_label_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0margmax_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margmax_label_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/sem6/NLP/Neural-Net-Constituent-Parser/vocabulary.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
