{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-e2d704c98a9e>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-e2d704c98a9e>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    import ../trees\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# SSM's code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "import trees\n",
    "import evaluate\n",
    "import vocabulary\n",
    "\n",
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "def augment(scores, oracle_index):\n",
    "    \"\"\"Adds the hamming loss to the wrong labels\"\"\"\n",
    "    print(\"(Augment function) Size of scores tensor: {}\".format(scores.size()))\n",
    "    shape = list(scores.size())[0]\n",
    "    increment = torch.ones(shape)\n",
    "    # increment = torch.ones(shape,1)\n",
    "    increment[oracle_index] = 0\n",
    "    return scores + Variable(increment)\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        dims = [self.input_dim] + self.hidden_dims + [self.output_dim]\n",
    "        for prev_dim, next_dim in zip(dims, dims[1:]):\n",
    "            self.weights.append(nn.Parameter(torch.zeros(prev_dim, next_dim)))\n",
    "            self.biases.append(nn.Parameter(torch.zeros(next_dim,1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(x)\n",
    "            x = torch.matmul(weight.t(),x)\n",
    "            print(x)\n",
    "            x = torch.add(bias,x)\n",
    "            if i < len(self.weights) - 1:\n",
    "                x = self.relu(x)\n",
    "            print(x)\n",
    "        return x\n",
    "\n",
    "class TopDownParser(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag_vocab,\n",
    "            word_vocab,\n",
    "            label_vocab,\n",
    "            tag_embedding_dim,\n",
    "            word_embedding_dim,\n",
    "            lstm_layers,\n",
    "            lstm_dim,\n",
    "            label_hidden_dim,\n",
    "            split_hidden_dim,\n",
    "            dropout,\n",
    "    ):\n",
    "        super(TopDownParser, self).__init__()\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.tag_embedding_dim = tag_embedding_dim\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.label_hidden_dim = label_hidden_dim\n",
    "        self.split_hidden_dim = split_hidden_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.tag_embeddings = nn.Embedding(tag_vocab.size, tag_embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding(word_vocab.size, word_embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.tag_embedding_dim + self.word_embedding_dim,\n",
    "            2 * self.lstm_dim,\n",
    "            num_layers=self.lstm_layers,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=True\n",
    "            )\n",
    "\n",
    "        self.f_label = Feedforward(2 * lstm_dim, [label_hidden_dim], label_vocab.size)\n",
    "        self.f_split = Feedforward(2 * lstm_dim, [split_hidden_dim], 1)\n",
    "\n",
    "    def forward(self, sentence, gold=None, explore=True):\n",
    "        is_train = gold is not None\n",
    "\n",
    "        if is_train:\n",
    "            #enable dropout in lstm\n",
    "            pass\n",
    "        else:\n",
    "            #disable dropout in lstm\n",
    "            torch.no_grad()\n",
    "\n",
    "        embeddings = []\n",
    "        for tag, word in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            tag_embedding = self.tag_embeddings[self.tag_vocab.index(tag)]\n",
    "            if word not in (START, STOP):\n",
    "                count = self.word_vocab.count(word)\n",
    "                if not count or (is_train and np.random.rand() < 1 / (1 + count)):\n",
    "                    word = UNK\n",
    "            word_embedding = self.word_embeddings[self.word_vocab.index(word)]\n",
    "            embeddings.append(torch.cat([tag_embedding, word_embedding],1))\n",
    "\n",
    "        lstm_outputs = self.lstm(embeddings)\n",
    "\n",
    "        @functools.lru_cache(maxsize=None)\n",
    "        def get_span_encoding(left, right):\n",
    "            forward = (\n",
    "                lstm_outputs[right][:self.lstm_dim] -\n",
    "                lstm_outputs[left][:self.lstm_dim])\n",
    "            backward = (\n",
    "                lstm_outputs[left + 1][self.lstm_dim:] -\n",
    "                lstm_outputs[right + 1][self.lstm_dim:])\n",
    "            return torch.cat([forward, backward],1)\n",
    "\n",
    "        def helper(left, right):\n",
    "            label_scores = self.f_label(get_span_encoding(left, right))\n",
    "\n",
    "            if is_train:\n",
    "                oracle_label = gold.oracle_label(left, right)\n",
    "                oracle_label_index = self.label_vocab.index(oracle_label)\n",
    "                label_scores = augment(label_scores, oracle_label_index)\n",
    "\n",
    "            label_scores_np = label_scores.numpy()\n",
    "            argmax_label_index = int(\n",
    "                label_scores_np.argmax() if right - left < len(sentence) else\n",
    "                label_scores_np[1:].argmax() + 1)\n",
    "            argmax_label = self.label_vocab.value(argmax_label_index)\n",
    "\n",
    "            if is_train:\n",
    "                label = argmax_label if explore else oracle_label\n",
    "                label_loss = (\n",
    "                    label_scores[argmax_label_index] -\n",
    "                    label_scores[oracle_label_index]\n",
    "                    if argmax_label != oracle_label else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                label = argmax_label\n",
    "                label_loss = label_scores[argmax_label_index]\n",
    "\n",
    "            if right - left == 1:\n",
    "                tag, word = sentence[left]\n",
    "                tree = trees.LeafParseNode(left, tag, word)\n",
    "                if label:\n",
    "                    tree = trees.InternalParseNode(label, [tree])\n",
    "                return [tree], label_loss\n",
    "\n",
    "            left_encodings = []\n",
    "            right_encodings = []\n",
    "            for split in range(left + 1, right):\n",
    "                left_encodings.append(get_span_encoding(left, split))\n",
    "                right_encodings.append(get_span_encoding(split, right))\n",
    "\n",
    "            #need to check dimensions here\n",
    "            print(\"(Helper function) Dimension of split encodings left: {}\".format(left_encodings[0].size()))\n",
    "            left_scores = self.f_split(left_encodings,1)\n",
    "            right_scores = self.f_split(right_encodings,1)\n",
    "            split_scores = left_scores + right_scores\n",
    "            split_scores = split_scores.view(-1, len(left_encodings), 1)\n",
    "            print(\"(Helper function) Dimension of split scores: {}\".format(split_scores.size()))\n",
    "\n",
    "            if is_train:\n",
    "                oracle_splits = gold.oracle_splits(left, right)\n",
    "                oracle_split = min(oracle_splits)\n",
    "                oracle_split_index = oracle_split - (left + 1)\n",
    "                split_scores = augment(split_scores, oracle_split_index)\n",
    "\n",
    "            split_scores_np = split_scores.numpy()\n",
    "            argmax_split_index = int(split_scores_np.argmax())\n",
    "            argmax_split = argmax_split_index + (left + 1)\n",
    "\n",
    "            if is_train:\n",
    "                split = argmax_split if explore else oracle_split\n",
    "                split_loss = (\n",
    "                    split_scores[argmax_split_index] -\n",
    "                    split_scores[oracle_split_index]\n",
    "                    if argmax_split != oracle_split else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                split = argmax_split\n",
    "                split_loss = split_scores[argmax_split_index]\n",
    "\n",
    "            left_trees, left_loss = helper(left, split)\n",
    "            right_trees, right_loss = helper(split, right)\n",
    "\n",
    "            children = left_trees + right_trees\n",
    "            if label:\n",
    "                children = [trees.InternalParseNode(label, children)]\n",
    "\n",
    "            return children, label_loss + split_loss + left_loss + right_loss\n",
    "\n",
    "        children, loss = helper(0, len(sentence))\n",
    "        assert len(children) == 1\n",
    "        tree = children[0]\n",
    "        if is_train and not explore:\n",
    "            assert gold.convert().linearize() == tree.convert().linearize()\n",
    "        return tree, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "def format_elapsed(start_time):\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    days, hours = divmod(hours, 24)\n",
    "    elapsed_string = \"{}h{:02}m{:02}s\".format(hours, minutes, seconds)\n",
    "    if days > 0:\n",
    "        elapsed_string = \"{}d{}\".format(days, elapsed_string)\n",
    "    return elapsed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_treebank = trees.load_trees(\"data/02-21.10way.clean\")\n",
    "    dev_treebank = trees.load_trees(\"data/22.auto.clean\")\n",
    "    train_parse = [tree.convert() for tree in train_treebank]\n",
    "    \n",
    "    tag_vocab = vocabulary.Vocabulary()\n",
    "    tag_vocab.index(parse.START)\n",
    "    tag_vocab.index(parse.STOP)\n",
    "\n",
    "    word_vocab = vocabulary.Vocabulary()\n",
    "    word_vocab.index(parse.START)\n",
    "    word_vocab.index(parse.STOP)\n",
    "    word_vocab.index(parse.UNK)\n",
    "\n",
    "    label_vocab = vocabulary.Vocabulary()\n",
    "    label_vocab.index(())\n",
    "\n",
    "    for tree in train_parse:\n",
    "        nodes = [tree]\n",
    "        while nodes:\n",
    "            node = nodes.pop()\n",
    "            if isinstance(node, trees.InternalParseNode):\n",
    "                label_vocab.index(node.label)\n",
    "                nodes.extend(reversed(node.children))\n",
    "            else:\n",
    "                tag_vocab.index(node.tag)\n",
    "                word_vocab.index(node.word)\n",
    "\n",
    "    tag_vocab.freeze()\n",
    "    word_vocab.freeze()\n",
    "    label_vocab.freeze()\n",
    "    \n",
    "    def print_vocabulary(name, vocab):\n",
    "        special = {parse.START, parse.STOP, parse.UNK}\n",
    "        print(\"{} ({:,}): {}\".format(\n",
    "            name, vocab.size,\n",
    "            sorted(value for value in vocab.values if value in special) +\n",
    "            sorted(value for value in vocab.values if value not in special)))\n",
    "\n",
    "    if True:\n",
    "        print_vocabulary(\"Tag\", tag_vocab)\n",
    "        print_vocabulary(\"Word\", word_vocab)\n",
    "        print_vocabulary(\"Label\", label_vocab)\n",
    "        \n",
    "    parser = TopDownParser(\n",
    "        tag_vocab,\n",
    "        word_vocab,\n",
    "        label_vocab,\n",
    "        50, #tag_embedding_dim,\n",
    "        100, #word_embedding_dim,\n",
    "        2, #lstm_layers,\n",
    "        10, #250, #lstm_dim,\n",
    "        10, #250, #label_hidden_dim,\n",
    "        10, #250, #split_hidden_dim,\n",
    "        0.4 #dropout\n",
    "    )\n",
    "    parser.cuda()\n",
    "    \n",
    "    optimizer = optim.Adam(parser.parameters())\n",
    "    \n",
    "    total_processed = 0\n",
    "    current_processed = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in itertools.count(start=1):\n",
    "        if 10 is not None and epoch > 10:\n",
    "            break\n",
    "\n",
    "        np.random.shuffle(train_parse)\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for start_index in range(0, len(train_parse), args.batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            parser.train()\n",
    "            batch_losses = []\n",
    "            for tree in train_parse[start_index:start_index + 10]:\n",
    "                sentence = [(leaf.tag, leaf.word) for leaf in tree.leaves()]\n",
    "                _, loss = parser.forward(sentence, tree, True)\n",
    "                batch_losses.append(loss)\n",
    "                total_processed += 1\n",
    "                current_processed += 1\n",
    "\n",
    "            batch_loss = torch.stack(batch_losses).mean()\n",
    "            batch_loss_value = batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"epoch {:,} \"\n",
    "                \"batch {:,}/{:,} \"\n",
    "                \"processed {:,} \"\n",
    "                \"batch-loss {:.4f} \"\n",
    "                \"epoch-elapsed {} \"\n",
    "                \"total-elapsed {}\".format(\n",
    "                    epoch,\n",
    "                    start_index // args.batch_size + 1,\n",
    "                    int(np.ceil(len(train_parse) / args.batch_size)),\n",
    "                    total_processed,\n",
    "                    batch_loss_value,\n",
    "                    format_elapsed(epoch_start_time),\n",
    "                    format_elapsed(start_time),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-68274d7e071c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-80492751e386>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_treebank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/02-21.10way.clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdev_treebank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/22.auto.clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_treebank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trees' is not defined"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
