{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSM's code\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "import trees\n",
    "import evaluate\n",
    "import vocabulary\n",
    "\n",
    "START = \"<START>\"\n",
    "STOP = \"<STOP>\"\n",
    "UNK = \"<UNK>\"\n",
    "# device = torch.device(\"cuda\")\n",
    "\n",
    "def augment(scores, oracle_index):\n",
    "    \"\"\"Adds the hamming loss to the wrong labels\"\"\"\n",
    "    print(\"(Augment function) Size of scores tensor: {}\".format(scores.size()))\n",
    "    shape = list(scores.size())[0]\n",
    "    increment = torch.ones(shape)\n",
    "    # increment = torch.ones(shape,1)\n",
    "    increment[oracle_index] = 0\n",
    "    return scores + Variable(increment)\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dim = output_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        dims = [self.input_dim] + self.hidden_dims + [self.output_dim]\n",
    "        for prev_dim, next_dim in zip(dims, dims[1:]):\n",
    "            self.weights.append(nn.Parameter(torch.zeros(prev_dim, next_dim)))\n",
    "            self.biases.append(nn.Parameter(torch.zeros(next_dim,1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(\"Size of x: {}\".format(x.size()))\n",
    "            print(\"Size of weight: {}\".format(weight.size()))\n",
    "            x = torch.matmul(weight.t(),x)\n",
    "            print(x)\n",
    "            x = torch.add(bias,x)\n",
    "            if i < len(self.weights) - 1:\n",
    "                x = self.relu(x)\n",
    "            print(x)\n",
    "        return x\n",
    "\n",
    "class TopDownParser(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            tag_vocab,\n",
    "            word_vocab,\n",
    "            label_vocab,\n",
    "            tag_embedding_dim,\n",
    "            word_embedding_dim,\n",
    "            lstm_layers,\n",
    "            lstm_dim,\n",
    "            label_hidden_dim,\n",
    "            split_hidden_dim,\n",
    "            dropout,\n",
    "    ):\n",
    "        super(TopDownParser, self).__init__()\n",
    "        self.tag_vocab = tag_vocab\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.tag_embedding_dim = tag_embedding_dim\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.label_hidden_dim = label_hidden_dim\n",
    "        self.split_hidden_dim = split_hidden_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.tag_embeddings = nn.Embedding(tag_vocab.size, tag_embedding_dim)\n",
    "        self.word_embeddings = nn.Embedding(word_vocab.size, word_embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.tag_embedding_dim + self.word_embedding_dim,\n",
    "            2 * self.lstm_dim,\n",
    "            num_layers=self.lstm_layers,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=True\n",
    "            )\n",
    "\n",
    "        self.f_label = Feedforward(2 * lstm_dim, [label_hidden_dim], label_vocab.size)\n",
    "        self.f_split = Feedforward(2 * lstm_dim, [split_hidden_dim], 1)\n",
    "\n",
    "    def forward(self, sentence, gold=None, explore=True):\n",
    "        is_train = gold is not None\n",
    "\n",
    "        if is_train:\n",
    "            #enable dropout in lstm\n",
    "            pass\n",
    "        else:\n",
    "            #disable dropout in lstm\n",
    "            torch.no_grad()\n",
    "\n",
    "        indices = []\n",
    "        for tag, word in [(START, START)] + sentence + [(STOP, STOP)]:\n",
    "            tag_index = self.tag_vocab.index(tag)\n",
    "            if word not in (START, STOP):\n",
    "                count = self.word_vocab.count(word)\n",
    "                if not count or (is_train and np.random.rand() < 1 / (1 + count)):\n",
    "                    word = UNK\n",
    "            word_index = self.word_vocab.index(word)\n",
    "            indices.append([tag_index, word_index])\n",
    "\n",
    "        indices = torch.LongTensor(indices).t()\n",
    "        print(indices)\n",
    "        embeddings = torch.cat([self.tag_embeddings(indices[0]), \n",
    "                                self.word_embeddings(indices[1])],\n",
    "                                -1)\n",
    "        print(\"Embedding dim: \",embeddings.size())\n",
    "#         embeddings = embeddings.to(device)\n",
    "        lstm_outputs, _ = self.lstm(embeddings.unsqueeze(1))\n",
    "        lstm_outputs = lstm_outputs.squeeze()\n",
    "\n",
    "        @functools.lru_cache(maxsize=None)\n",
    "        def get_span_encoding(left, right):\n",
    "            print(\"LSTM output dimension {}:\".format(lstm_outputs.size()))\n",
    "            forward = (\n",
    "                lstm_outputs[right][:self.lstm_dim] -\n",
    "                lstm_outputs[left][:self.lstm_dim])\n",
    "            backward = (\n",
    "                lstm_outputs[left + 1][self.lstm_dim:] -\n",
    "                lstm_outputs[right + 1][self.lstm_dim:])\n",
    "            print(\"Forward dim: {}\".format(forward.size()))\n",
    "            print(\"Backward dim: {}\".format(backward.size()))\n",
    "            return torch.tensor(torch.cat((forward,backward),0).tolist())\n",
    "#             return torch.cat([forward, backward])\n",
    "\n",
    "        def helper(left, right):\n",
    "            label_scores = self.f_label(get_span_encoding(left, right))\n",
    "\n",
    "            if is_train:\n",
    "                oracle_label = gold.oracle_label(left, right)\n",
    "                oracle_label_index = self.label_vocab.index(oracle_label)\n",
    "                label_scores = augment(label_scores, oracle_label_index)\n",
    "\n",
    "            label_scores_np = label_scores.numpy()\n",
    "            argmax_label_index = int(\n",
    "                label_scores_np.argmax() if right - left < len(sentence) else\n",
    "                label_scores_np[1:].argmax() + 1)\n",
    "            argmax_label = self.label_vocab.value(argmax_label_index)\n",
    "\n",
    "            if is_train:\n",
    "                label = argmax_label if explore else oracle_label\n",
    "                label_loss = (\n",
    "                    label_scores[argmax_label_index] -\n",
    "                    label_scores[oracle_label_index]\n",
    "                    if argmax_label != oracle_label else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                label = argmax_label\n",
    "                label_loss = label_scores[argmax_label_index]\n",
    "\n",
    "            if right - left == 1:\n",
    "                tag, word = sentence[left]\n",
    "                tree = trees.LeafParseNode(left, tag, word)\n",
    "                if label:\n",
    "                    tree = trees.InternalParseNode(label, [tree])\n",
    "                return [tree], label_loss\n",
    "\n",
    "            left_encodings = []\n",
    "            right_encodings = []\n",
    "            for split in range(left + 1, right):\n",
    "                left_encodings.append(get_span_encoding(left, split))\n",
    "                right_encodings.append(get_span_encoding(split, right))\n",
    "\n",
    "            #need to check dimensions here\n",
    "            print(\"(Helper function) Dimension of split encodings left: {}\".format(left_encodings[0].size()))\n",
    "            left_scores = self.f_split(left_encodings,1)\n",
    "            right_scores = self.f_split(right_encodings,1)\n",
    "            split_scores = left_scores + right_scores\n",
    "            split_scores = split_scores.view(-1, len(left_encodings), 1)\n",
    "            print(\"(Helper function) Dimension of split scores: {}\".format(split_scores.size()))\n",
    "\n",
    "            if is_train:\n",
    "                oracle_splits = gold.oracle_splits(left, right)\n",
    "                oracle_split = min(oracle_splits)\n",
    "                oracle_split_index = oracle_split - (left + 1)\n",
    "                split_scores = augment(split_scores, oracle_split_index)\n",
    "\n",
    "            split_scores_np = split_scores.numpy()\n",
    "            argmax_split_index = int(split_scores_np.argmax())\n",
    "            argmax_split = argmax_split_index + (left + 1)\n",
    "\n",
    "            if is_train:\n",
    "                split = argmax_split if explore else oracle_split\n",
    "                split_loss = (\n",
    "                    split_scores[argmax_split_index] -\n",
    "                    split_scores[oracle_split_index]\n",
    "                    if argmax_split != oracle_split else Variable(torch.zeros(1)))\n",
    "            else:\n",
    "                split = argmax_split\n",
    "                split_loss = split_scores[argmax_split_index]\n",
    "\n",
    "            left_trees, left_loss = helper(left, split)\n",
    "            right_trees, right_loss = helper(split, right)\n",
    "\n",
    "            children = left_trees + right_trees\n",
    "            if label:\n",
    "                children = [trees.InternalParseNode(label, children)]\n",
    "\n",
    "            return children, label_loss + split_loss + left_loss + right_loss\n",
    "\n",
    "        children, loss = helper(0, len(sentence))\n",
    "        assert len(children) == 1\n",
    "        tree = children[0]\n",
    "        if is_train and not explore:\n",
    "            assert gold.convert().linearize() == tree.convert().linearize()\n",
    "        return tree, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "def format_elapsed(start_time):\n",
    "    elapsed_time = int(time.time() - start_time)\n",
    "    minutes, seconds = divmod(elapsed_time, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    days, hours = divmod(hours, 24)\n",
    "    elapsed_string = \"{}h{:02}m{:02}s\".format(hours, minutes, seconds)\n",
    "    if days > 0:\n",
    "        elapsed_string = \"{}d{}\".format(days, elapsed_string)\n",
    "    return elapsed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    train_treebank = trees.load_trees(\"../data/02-21.10way.clean\")\n",
    "#     dev_treebank = trees.load_trees(\"../data/22.auto.clean\")\n",
    "    train_parse = [tree.convert() for tree in train_treebank]\n",
    "    \n",
    "    tag_vocab = vocabulary.Vocabulary()\n",
    "    tag_vocab.index(START)\n",
    "    tag_vocab.index(STOP)\n",
    "\n",
    "    word_vocab = vocabulary.Vocabulary()\n",
    "    word_vocab.index(START)\n",
    "    word_vocab.index(STOP)\n",
    "    word_vocab.index(UNK)\n",
    "\n",
    "    label_vocab = vocabulary.Vocabulary()\n",
    "    label_vocab.index(())\n",
    "\n",
    "    for tree in train_parse:\n",
    "        nodes = [tree]\n",
    "        while nodes:\n",
    "            node = nodes.pop()\n",
    "            if isinstance(node, trees.InternalParseNode):\n",
    "                label_vocab.index(node.label)\n",
    "                nodes.extend(reversed(node.children))\n",
    "            else:\n",
    "                tag_vocab.index(node.tag)\n",
    "                word_vocab.index(node.word)\n",
    "\n",
    "    tag_vocab.freeze()\n",
    "    word_vocab.freeze()\n",
    "    label_vocab.freeze()\n",
    "    \n",
    "    def print_vocabulary(name, vocab):\n",
    "        special = {START, STOP, UNK}\n",
    "        print(\"{} ({:,}): {}\".format(\n",
    "            name, vocab.size,\n",
    "            sorted(value for value in vocab.values if value in special) +\n",
    "            sorted(value for value in vocab.values if value not in special)))\n",
    "\n",
    "    if False:\n",
    "        print_vocabulary(\"Tag\", tag_vocab)\n",
    "        print_vocabulary(\"Word\", word_vocab)\n",
    "        print_vocabulary(\"Label\", label_vocab)\n",
    "        \n",
    "    parser = TopDownParser(\n",
    "        tag_vocab,\n",
    "        word_vocab,\n",
    "        label_vocab,\n",
    "        50, #tag_embedding_dim,\n",
    "        100, #word_embedding_dim,\n",
    "        2, #lstm_layers,\n",
    "        10, #250, #lstm_dim,\n",
    "        10, #250, #label_hidden_dim,\n",
    "        10, #250, #split_hidden_dim,\n",
    "        0.4 #dropout\n",
    "    )\n",
    "#     parser.cuda()\n",
    "    learning_rate = 1e-4\n",
    "    optimizer = optim.Adam(parser.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_processed = 0\n",
    "    current_processed = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in itertools.count(start=1):\n",
    "        if 10 is not None and epoch > 10:\n",
    "            break\n",
    "\n",
    "        np.random.shuffle(train_parse)\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        for start_index in range(0, len(train_parse), 10):\n",
    "            optimizer.zero_grad()\n",
    "            parser.train()\n",
    "            batch_losses = []\n",
    "            for tree in train_parse[start_index:start_index + 10]:\n",
    "                sentence = [(leaf.tag, leaf.word) for leaf in tree.leaves()]\n",
    "                _, loss = parser.forward(sentence, tree, True)\n",
    "                batch_losses.append(loss)\n",
    "                total_processed += 1\n",
    "                current_processed += 1\n",
    "\n",
    "            batch_loss = torch.stack(batch_losses).mean()\n",
    "            batch_loss_value = batch_loss.data[0]\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\n",
    "                \"epoch {:,} \"\n",
    "                \"batch {:,}/{:,} \"\n",
    "                \"processed {:,} \"\n",
    "                \"batch-loss {:.4f} \"\n",
    "                \"epoch-elapsed {} \"\n",
    "                \"total-elapsed {}\".format(\n",
    "                    epoch,\n",
    "                    start_index // args.batch_size + 1,\n",
    "                    int(np.ceil(len(train_parse) / 10)),\n",
    "                    total_processed,\n",
    "                    batch_loss_value,\n",
    "                    format_elapsed(epoch_start_time),\n",
    "                    format_elapsed(start_time),\n",
    "                )\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    3,    4,    6,    6,   18,    3,    5,    6,   22,    6,    2,\n",
      "            5,    5,   12,   10,   38,    5,    5,   16,   15,   12,    2,    5,\n",
      "            5,   12,   10,   38,    5,    5,   16,   21,    1],\n",
      "        [   0,   10, 9134, 3032,  163,   52,   22,   63, 1185,   79, 3972,    8,\n",
      "         9135,  413, 9136,   18, 1597, 9137,  413,   31,   74, 9138,    8,    2,\n",
      "          413, 9136,   18, 1597,    2,  413,   31,   44,    1]])\n",
      "tensor([[-0.9977, -0.2757,  0.9833,  ...,  0.7854, -0.0634,  1.0863],\n",
      "        [ 0.0723,  0.1435, -0.1865,  ...,  0.4057, -1.0587,  1.3166],\n",
      "        [ 1.3071,  0.8246,  0.6808,  ...,  0.7034, -0.5600, -0.1644],\n",
      "        ...,\n",
      "        [-1.3308,  1.9109, -0.3381,  ...,  1.4852,  1.8280, -1.1081],\n",
      "        [ 0.1945, -0.4732, -1.6182,  ...,  0.6734,  0.3810, -1.6288],\n",
      "        [ 0.3550,  0.5191, -1.6699,  ..., -1.4625,  1.3029, -0.1755]],\n",
      "       grad_fn=<CatBackward>)\n",
      "LSTM output dimension torch.Size([33, 40]):\n",
      "Forward dim: torch.Size([10])\n",
      "Backward dim: torch.Size([30])\n",
      "Size of x: torch.Size([40])\n",
      "Size of weight: torch.Size([20, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, [10 x 20], [40] at /tmp/pip-req-build-4baxydiv/aten/src/TH/generic/THTensorMath.cpp:348",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-68274d7e071c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-884d003a7b06>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_parse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mtotal_processed\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d6596f60f01d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence, gold, explore)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msplit_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mleft_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d6596f60f01d>\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mlabel_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_span_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-d6596f60f01d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size of x: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Size of weight: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, [10 x 20], [40] at /tmp/pip-req-build-4baxydiv/aten/src/TH/generic/THTensorMath.cpp:348"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
